{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2609c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting keybert\n",
      "  Using cached keybert-0.6.0-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from keybert) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from keybert) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from keybert) (1.0.2)\n",
      "Collecting rich>=10.4.0\n",
      "  Using cached rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Collecting typing-extensions<5.0,>=4.0.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rich>=10.4.0->keybert) (0.9.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (4.62.3)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (0.5.0+cu92)\n",
      "Collecting transformers<5.0.0,>=3.1.0\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 KB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2022.1.18)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.0.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
      "Installing collected packages: tokenizers, typing-extensions, torch, rich, huggingface-hub, transformers, torchvision, keybert\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc1\n",
      "    Uninstalling tokenizers-0.8.1rc1:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0+cu92\n",
      "    Uninstalling torch-1.4.0+cu92:\n",
      "      Successfully uninstalled torch-1.4.0+cu92\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 9.8.2\n",
      "    Uninstalling rich-9.8.2:\n",
      "      Successfully uninstalled rich-9.8.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.0.2\n",
      "    Uninstalling transformers-3.0.2:\n",
      "      Successfully uninstalled transformers-3.0.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0+cu92\n",
      "    Uninstalling torchvision-0.5.0+cu92:\n",
      "      Successfully uninstalled torchvision-0.5.0+cu92\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flair 0.11.3 requires sentencepiece==0.1.95, but you have sentencepiece 0.1.96 which is incompatible.\n",
      "datasets 1.9.0 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.10.0 which is incompatible.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.24.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.10.0 keybert-0.6.0 rich-12.6.0 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.22.2 typing-extensions-4.4.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47755244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f8f0d4c82b42d8b6f643797bdff655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "kw_model = KeyBERT(model=model_name)\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "249a2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "green tea\n",
      "[('tea', 0.8191), ('green', 0.4961)]\n",
      "--------------------\n",
      "--------------------\n",
      "green tea can\n",
      "[('tea', 0.621), ('can', 0.5149), ('green', 0.4287)]\n",
      "--------------------\n",
      "--------------------\n",
      "korean grill\n",
      "[('grill', 0.7139), ('korean', 0.6055)]\n",
      "--------------------\n",
      "--------------------\n",
      "milk tea\n",
      "[('tea', 0.8023), ('milk', 0.6853)]\n",
      "--------------------\n",
      "--------------------\n",
      "milk tea can\n",
      "[('tea', 0.6077), ('milk', 0.561), ('can', 0.4359)]\n",
      "--------------------\n",
      "--------------------\n",
      "korean skin care\n",
      "[('korean', 0.5543), ('skin', 0.5433), ('care', 0.1182)]\n",
      "--------------------\n",
      "--------------------\n",
      "Japan Cake\n",
      "[('cake', 0.7203), ('japan', 0.6956)]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "doc=['green tea','green tea can','korean grill','milk tea','milk tea can','korean skin care','Japan Cake']\n",
    "for i in doc:\n",
    "    keywords = kw_model.extract_keywords(i)\n",
    "    print('-'*20)\n",
    "    print(i)\n",
    "    print(kw_model.extract_keywords(i, keyphrase_ngram_range=(1, 1), stop_words=None))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49003b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e459452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['绿', ' ', '茶']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_zh(\"绿 茶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "990d2d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "韩国烤架\n",
      "[('韩国', 0.8639), ('烤架', 0.539)]\n",
      "--------------------\n",
      "--------------------\n",
      "日本蛋糕\n",
      "[('日本', 0.7431), ('蛋糕', 0.5195)]\n",
      "--------------------\n",
      "--------------------\n",
      "素食银耳\n",
      "[('银耳', 0.7552), ('素食', 0.7362)]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "doc=[\"韩国烤架\",\"日本蛋糕\",\"素食银耳\"]\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "for i in doc:\n",
    "    keywords = kw_model.extract_keywords(i,keyphrase_ngram_range=(1, 3),vectorizer=vectorizer)\n",
    "    print('-'*20)\n",
    "    print(i)\n",
    "    print(keywords)\n",
    "#     print(kw_model.extract_keywords(i, keyphrase_ngram_range=(1, 1), stop_words=None))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c6c1029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('绿色茶shui', 1.0), ('', 0.2573)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "doc=\"绿色茶shui\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(0, 10), stop_words=None,top_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
