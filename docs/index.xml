<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AWS Datalab- 基于阅读行为的用户归因分析 on Datalab Workshop</title><link>/</link><description>Recent content in AWS Datalab- 基于阅读行为的用户归因分析 on Datalab Workshop</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>1.1 KeyBert</title><link>/01introduction/keybert.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/01introduction/keybert.html</guid><description>使用场景 KeyBERT是一种小型且容易上手使用的关键字提取技术，它利用BERT嵌入来创建与文档最相似的关键词和关键字短语。
尽管我们已经有许多可用于关键字生成的方法(例如，Rake、YAKE!、TF-IDF等)，但是我们还是需要创建一种非常高效并且功能强大的方法来提取关键字和关键字。 这就是KeyBERT诞生的初衷！ 它使用BERT嵌入和简单的余弦相似性来查找文档中与文档本身最相似的子短语。
首先，使用BERT提取文档向量(嵌入)以获取文档级表示。 然后，针对N元语法词/短语提取词向量。 最后，我们使用余弦相似度来查找与文档最相似的词/短语。 然后，可以将最相似的词识定义为最能描述整个文档的词。
KeyBERT可能不是唯一的提取关键词的方法，它的定位主要是一种用于创建关键字和关键词的快速简便的方法。 尽管有很多出色的论文和解决方案都使用BERT嵌入，但是很少有直接基于BERT的解决方案，该工具无需从头开始进行训练模型，初学者也可直接使用
原理介绍 动手实验 直接使用，代码：https://github.com/jackie930/financial-Forecast-RCA/blob/main/code/keyword_extraction/KeyBert/demo.ipynb 部署成推理节点使用， 代码：https://github.com/jackie930/financial-Forecast-RCA/blob/main/code/keyword_extraction/KeyBert/deploy.ipynb 参考 代码：https://github.com/MaartenGr/KeyBERT paper：https://www.preprints.org/manuscript/201908.0073/download/final_file</description></item><item><title>1.2 UIE</title><link>/01introduction/uie.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/01introduction/uie.html</guid><description>使用场景 信息抽取（IE）是一个从文本到结构的转换过程。常见的实体、关系、事件分别采取Span、Triplet、Record形式的异构结构。
曾几何时，当我们面对各种复杂多样的IE任务，我们总会造各式各样IE模型的轮子，来满足不同复杂任务的多变需求。
真实的情况是：针对不同任务设定，需要针对特定领域schema建模，不同IE模型被单个训练、不共享，一个公司可能需要管理众多IE模型。
因而，随着NLP的发展，统一/通用的IE是一个众望所归的模型。
UIE来自2022ACL, 他可以做到：
统一地建模不同的IE任务 自适应地生成目标结构 从不同的知识来源统一学习通用的信息抽取能力。 原理介绍 UIE提出的统一生成框架，基于T5模型进行了IE预训练，在实体、关系、事件和情感等4个信息抽取任务、13个数据集的全监督、低资源和少样本设置下均取得了SOTA性能。
优势 使用简单：用户可以使用自然语言自定义抽取目标，无需训练即可统一抽取输入文本中的对应信息。实现开箱即用，并满足各类信息抽取需求。 降本增效：以往的信息抽取技术需要大量标注数据才能保证信息抽取的效果，为了提高开发过程中的开发效率，减少不必要的重复工作时间，开放域信息抽取可以实现零样本（zero-shot）或者少样本（few-shot）抽取，大幅度降低标注数据依赖，在降低成本的同时，还提升了效果。 效果领先：开放域信息抽取在多种场景，多种任务上，均有不俗的表现。 动手实验 https://github.com/jackie930/financial-Forecast-RCA/blob/main/code/keyword_extraction/UIE/uie_byos_gpu.ipynb
参考 uie 源码 https://github.com/universal-ie/UIE paddlenlp https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie paper https://arxiv.org/pdf/2203.12277.pdf</description></item><item><title>1.3 T5-Prompt</title><link>/01introduction/t5-prompt.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/01introduction/t5-prompt.html</guid><description>NLP 模型的发展 过去许多机器学习方法是基于全监督学习 (fully supervised learning) 的.
由于监督学习需要大量的数据学习性能优异的模型, 而在 NLP 中大规模训练数据(指为特定任务而标注好的数据)是不足的, 因此在深度学习出现之前研究者通常聚焦于特征工程 (feature engineering), 即利用领域知识从数据中提取好的特征;
在深度学习出现之后, 由于特征可以从数据中习得, 因此研究者转向了结构工程 (architecture engineering), 即通过通过设计一个合适的网络结构来把归纳偏置 (inductive bias) 引入模型中, 从而有利于学习好的特征.
在 2017-2019 年, NLP 模型开始转向一个新的模式 (BERT), 即预训练 + 微调 (pre-train and fine-tune). 在这个模式中, 先用一个固定的结构预训练一个语言模型 (language model, LM), 预训练的方式就是让模型补全上下文 (比如完形填空).
由于预训练不需要专家知识, 因此可以在网络上搜集的大规模文本上直接进行训练. 然后这个 LM 通过引入额外的参数或微调来适应到下游任务上. 此时研究者转向了 目标工程 (objective engineering), 即为预训练任务和微调任务设计更好的目标函数.
prompt learning 介绍 在做 objective engineering 的过程中, 研究者发现让下游任务的目标与预训练的目标对齐是有好的. 因此下游任务通过引入文本提示符 (textual prompt), 把原来的任务目标重构为与预训练模型一致的填空题.
比如一个输入 “I missed the bus today.</description></item><item><title>2.1 阅读行为分类模型介绍</title><link>/02textrank/text-classifier.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/02textrank/text-classifier.html</guid><description>阅读行为分类模型介绍 任务概述 用户浏览投资相关内容（新闻，feed， 报告 等） 并产生入金行为 基于用户浏览内容构建文本分类模型判断是否有入金行为 数据形式 已有数据包含用户和浏览内容的相关信息 数据的基本概况
数据包含4479 个用户， 20180 条数据 其中 1806 个用户具有入金行为 占比 40.6% 其中 4968 条数据包含入金行为 占比 24.6% 其中 2399 个用户 只有1条记录，占比 53%， 超过5 条数据的用户占比 13% 数据准备 数据清洗： 除去标点和数字 缺失值填充： 空格代替缺失值 异常值处理： 个别用户浏览超过800+条内容 文本分类模型搭建 基于BERT 与训练模型 创建文本分类模型。 基本架构如下
基于单条文本预测模型（a.k.a. BERT) 基于最近三条浏览文本的预测模型 （a.k.a, BERT+CNN, BERT+LSTM) 动手实验 https://github.com/jackie930/financial-Forecast-RCA/tree/main/code/textcls_rca/code</description></item><item><title>2.2 SHAP RCA</title><link>/02textrank/shap.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/02textrank/shap.html</guid><description>SHAP 归因模型介绍 概述 SHAP 归因模型分析每一个输入变量对与模型输出的影响大小
相关链接 SHAP 官方文档 External Link SHAP 论文 External Link SHAP 相关blog External Link 原理 SHAP 分析并计算 每个输入变量的对模型输出的影响大小 i.e., shaply-value 原理实践 假设 f 为文本分类模型。 该模型输入为文本，输出 [0,1] 举例 文本输入为： text= &amp;lsquo;suprising twits and turns&amp;rsquo;。 本输入中包含 4 个输入变量 F={superising, twists, and , turns} SHAP 模型 计算并输出 每一个变量的shaply-value 以变量 superising 为例， 下面展示shaply-value的具体计算过程 动手实验 https://github.com/jackie930/financial-Forecast-RCA/tree/main/code/textcls_rca/code</description></item></channel></rss>